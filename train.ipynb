{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "First we will download the tinyshakespeare dataset and examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-24 16:27:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-05-24 16:27:47 (16.1 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple character level tokenizer.\n",
    "\n",
    "We will create a simple character level tokenizer. Each unique character in the dataset becomes a token mapped to an integer value. In practice character level tokenizers are not used.\n",
    "Check out BPE tonezizers like SentencePiece and tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    \n",
    "    def char_to_index(self, ch):\n",
    "        return self.char_to_ix[ch]\n",
    "    \n",
    "    def index_to_char(self, ix):\n",
    "        return self.ix_to_char[ix]\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_index(ch) for ch in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.index_to_char(ix) for ix in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(chars)\n",
    "tokens = tokenizer.encode(\"Hello\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "block_size = 256\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i:i + block_size] for i in indices])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in indices])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32]) torch.Size([8, 32])\n",
      "X:\n",
      "sometimes you do blench from thi\n",
      "--------------------\n",
      "Y:\n",
      "ometimes you do blench from this\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_data)\n",
    "\n",
    "print(xb.shape, yb.shape)\n",
    "print(\"X:\")\n",
    "print(tokenizer.decode(xb[0].tolist()))\n",
    "print(\"-\" * 20)\n",
    "print(\"Y:\")\n",
    "print(tokenizer.decode(yb[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65]) tensor(4.6045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "3Pgvr-UzQkvbRf;pgfMy:ZNH bC-YI?d.eUcFCu DZ,fu\n",
      "'weH&gA3&gLKag-.xBmyQdz-X-:PcmSyxJHy,,Mm:UOoyyrEDRlP-U\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.embedding(idx) # (batch, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets = targets.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens = 25):\n",
    "        # idx: (batch, seq_len)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)                # (batch, seq_len, vocab_size) for logits\n",
    "\n",
    "            # this preserves the each sequence in the batch, and the vocab logits, but only the last token in each sequence\n",
    "            logits = logits[:, -1, :]                           # (batch, vocab_size)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                   # (batch, vocab_size)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)             # (batch, seq_len + 1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "logits = model.generate(idx, max_new_tokens=100)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teddy/anaconda3/envs/transLens/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.497288227081299\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_data)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Helokor wowed s, I'He averrosthandel 'so therand:\n",
      "LE htoucor a, he saclowave, thimup ge-mu!\n",
      "Thth,\n",
      "Goorre, ot: SCingt w, lle miz:\n",
      "I awisemitheacord thedimer my yorid Clkee yous dzef tineno t sathe re my, mo icl, d be thiou fe\n",
      "ASTe aisineforengheansto:\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "logits = model.generate(idx, max_new_tokens=250)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "\n",
    "        for step in range(100):\n",
    "            X, Y = get_batch(train_data if split == 'train' else test_data)\n",
    "            _, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data, test_data, n_steps=1000):\n",
    "    for step in range(n_steps):\n",
    "        model.train()\n",
    "        X, Y = get_batch(train_data)\n",
    "\n",
    "        logits, loss = model(X, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(estimate_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 2.4531653833389284, 'val': 2.483633105754852}\n",
      "{'train': 2.4555484461784363, 'val': 2.4888607978820803}\n",
      "{'train': 2.451585259437561, 'val': 2.485650296211243}\n",
      "{'train': 2.4584910559654234, 'val': 2.4844886326789855}\n",
      "{'train': 2.449352834224701, 'val': 2.4828339052200317}\n",
      "{'train': 2.4509687209129334, 'val': 2.4893119311332703}\n",
      "{'train': 2.4535897898674013, 'val': 2.490015721321106}\n",
      "{'train': 2.4513447952270506, 'val': 2.4801024127006532}\n",
      "{'train': 2.4534989309310915, 'val': 2.4852124357223513}\n",
      "{'train': 2.461052176952362, 'val': 2.482593240737915}\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_data, test_data, n_steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention\n",
    "\n",
    "We will start with a simple trick. We want every token to be \"aware\" of the context of the tokens that have occured before it (But not the tokens that follow).\n",
    "\n",
    "**Idea** - We could make each token an average of the tokens that preceded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0333, 0.0333, 0.0333,  ..., 0.0333, 0.0000, 0.0000],\n",
       "        [0.0323, 0.0323, 0.0323,  ..., 0.0323, 0.0323, 0.0000],\n",
       "        [0.0312, 0.0312, 0.0312,  ..., 0.0312, 0.0312, 0.0312]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
    "weights = weights / weights.sum(1, keepdim=True) \n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of weights that makes each token an average of the tokens that preceeded it. We can actually do the same thing with Softmax and tril (lower triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0333, 0.0333, 0.0333,  ..., 0.0333, 0.0000, 0.0000],\n",
       "        [0.0323, 0.0323, 0.0323,  ..., 0.0323, 0.0323, 0.0000],\n",
       "        [0.0312, 0.0312, 0.0312,  ..., 0.0312, 0.0312, 0.0312]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
    "weights = torch.zeros(block_size, block_size).to(device)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "weights = F.softmax(weights, dim=1)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-dependant Averaging for Self-Attention\n",
    "\n",
    "The information that is relevant to a given token when predicting future tokens is based only upon previous tokens. However, not all previously seen tokens carry equal importance. To that end, we parameterize this unequal importance using learned key and query vectors for each token. In this way, each token now is represented as a weighted average of all previous tokens, with more attention placed on some tokens vs others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "seq_len = 32\n",
    "x = torch.randn(batch_size, seq_len, vocab_size).to(device)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(vocab_size, head_size, bias=False).to(device)\n",
    "query = nn.Linear(vocab_size, head_size, bias=False).to(device)\n",
    "value = nn.Linear(vocab_size, head_size, bias=False).to(device)\n",
    "\n",
    "K = key(x)  # (batch, seq_len, head_size)\n",
    "Q = query(x)  # (batch, seq_len, head_size)\n",
    "V = value(x)  # (batch, seq_len, head_size)\n",
    "\n",
    "weights = Q @ K.transpose(-2, -1)  # (batch, seq_len, head_size) @ (batch, head_size, seq_len) -> (batch, seq_len, seq_len)\n",
    "\n",
    "tril = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "output = weights @ V  # (batch, seq_len, seq_len) @ (batch, seq_len, head_size) -> (batch, seq_len, head_size)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the attention.\n",
    "\n",
    "In the orignal *Attention is All You Need* paper, an additional scaling is applied, dividing by the square root of the dimensionality of the key embeddings.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This is done as a normlization step. If the key and query values are guassian with unit variance and mean 0 (are they?), multiplying the weighs by the values results in variance on the order of the head size. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K variance: 0.9817906022071838\n",
      "Q variance: 1.0200756788253784\n",
      "Weights variance: 16.122303009033203\n",
      "Weights variance after scaling: 1.0076439380645752\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(batch_size, seq_len, head_size).to(device)\n",
    "q = torch.randn(batch_size, seq_len, head_size).to(device)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(\"K variance:\", k.var().item())\n",
    "print(\"Q variance:\", q.var().item())\n",
    "\n",
    "print(\"Weights variance:\", weights.var().item())\n",
    "\n",
    "weights = weights / (head_size ** 0.5)\n",
    "\n",
    "print(\"Weights variance after scaling:\", weights.var().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does this matter?\n",
    "\n",
    "The reason is the Softmax function. If the input to a Softmax function has very high values and very low values it will actually converge to one-hot vectors\n",
    "\n",
    "(Can I find a paper about this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax of diffuse weights: tensor([0.1912, 0.1566, 0.5745, 0.0777], device='cuda:0')\n",
      "Softmax of high variance weights: tensor([0., 0., 1., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "diffuse_weights = torch.Tensor([0.1, -0.1, 1.2, -0.8]).to(device)\n",
    "\n",
    "print(\"Softmax of diffuse weights:\", F.softmax(diffuse_weights, dim=0))\n",
    "print(\"Softmax of high variance weights:\", F.softmax(diffuse_weights * 100, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_size, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.key = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embedding_size, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)).to(device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, vocab_size = x.shape\n",
    "\n",
    "        K = self.key(x)     # (batch, seq_len, embedding_size) -> (batch, seq_len, head_size)\n",
    "        Q = self.query(x)   # (batch, seq_len, embedding_size) -> (batch, seq_len, head_size)\n",
    "        V = self.value(x)   # (batch, seq_len, embedding_size) -> (batch, seq_len, head_size)\n",
    "\n",
    "        weights = Q @ K.transpose(-2, -1)                               # (batch, seq_len, head_size) @ (batch, head_size, seq_len) -> (batch, seq_len, seq_len)\n",
    "        \n",
    "        weights = weights.masked_fill(self.tril[:seq_len, :seq_len] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights / (self.head_size ** 0.5), dim=-1)\n",
    "\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        return weights @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_size = num_heads * head_size\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(embedding_size, head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(embed_size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        self.attn = MultiHeadAttention(head_size, num_heads)\n",
    "        self.ff = FeedForward(embedding_size)\n",
    "\n",
    "        self.ln1 = LayerNorm(embedding_size)\n",
    "        self.ln2 = LayerNorm(embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln1(x)) + x\n",
    "        x = self.ff(self.ln2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3.5761201047897337, 'val': 3.594433555603027}\n",
      "{'train': 1.583548265695572, 'val': 1.766865930557251}\n",
      "{'train': 1.3176550924777986, 'val': 1.569015531539917}\n",
      "{'train': 1.1914582431316376, 'val': 1.5287380754947661}\n",
      "{'train': 1.087228685617447, 'val': 1.5476093566417695}\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dim, num_heads=8) for _ in range(6)])\n",
    "\n",
    "        self.ln = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        batch_size, seq_len = idx.shape\n",
    "\n",
    "        embeddings = self.embedding(idx) # (batch, seq_len, embedding_dim)\n",
    "        positions = torch.arange(seq_len).to(device)\n",
    "\n",
    "        x = embeddings + self.position_embedding(positions) # (batch, seq_len, embedding_dim)\n",
    "\n",
    "        x = self.blocks(x) # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        x = self.ln(x) # (batch, seq_len, embedding_dim)\n",
    "\n",
    "        logits = self.lm_head(x) # (batch, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets = targets.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens = 25):\n",
    "        # idx: (batch, seq_len)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]     # (batch, block_size)\n",
    "\n",
    "            logits, _ = self(idx_cond)                # (batch, seq_len, vocab_size) for logits\n",
    "\n",
    "            # this preserves the each sequence in the batch, and the vocab logits, but only the last token in each sequence\n",
    "            logits = logits[:, -1, :]                           # (batch, vocab_size)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                   # (batch, vocab_size)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)             # (batch, seq_len + 1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size, embedding_dim=384)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "train(model, optimizer, train_data, test_data, n_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GLIENCENPUS:\n",
      "To all stay sie my, old we sunch budone.\n",
      "\n",
      "LICHV:\n",
      "Shich'd HuHas ad a diss doul'd off\n",
      "Be it his plits to-kereass:\n",
      "I'd with in porsw me I this all naw,\n",
      "Of the sam.\n",
      "\n",
      "VINGSCES:\n",
      "O, hou the umres man me sord it love,\n",
      "Thinne bey my Mereir, aband\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "logits = model.generate(idx, max_new_tokens=250)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transLens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
