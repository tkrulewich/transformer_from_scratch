{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "First we will download the tinyshakespeare dataset and examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-24 12:04:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.4’\n",
      "\n",
      "input.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-05-24 12:04:25 (18.2 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple character level tokenizer.\n",
    "\n",
    "We will create a simple character level tokenizer. Each unique character in the dataset becomes a token mapped to an integer value. In practice character level tokenizers are not used.\n",
    "Check out BPE tonezizers like SentencePiece and tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, chars):\n",
    "        self.char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    \n",
    "    def char_to_index(self, ch):\n",
    "        return self.char_to_ix[ch]\n",
    "    \n",
    "    def index_to_char(self, ix):\n",
    "        return self.ix_to_char[ix]\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_index(ch) for ch in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.index_to_char(ix) for ix in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(chars)\n",
    "tokens = tokenizer.encode(\"Hello\")\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "block_size = 32\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i:i + block_size] for i in indices])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in indices])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32]) torch.Size([8, 32])\n",
      "X:\n",
      "The valiant heart is not whipt o\n",
      "--------------------\n",
      "Y:\n",
      "he valiant heart is not whipt ou\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch(train_data)\n",
    "\n",
    "print(xb.shape, yb.shape)\n",
    "print(\"X:\")\n",
    "print(tokenizer.decode(xb[0].tolist()))\n",
    "print(\"-\" * 20)\n",
    "print(\"Y:\")\n",
    "print(tokenizer.decode(yb[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65]) tensor(4.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "N;MPjL\n",
      ".KyF,nIPXUrWfEqA.$Tm!FdfKNlkIxIjXOKRJ;SY:$VlRIgk'DRDeq?kVwxdlSzlhSy&c'GP,RWx'AdOjTrzPFwhuMsoo\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.embedding(idx) # (batch, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            targets = targets.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens = 25):\n",
    "        # idx: (batch, seq_len)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)                # (batch, seq_len, vocab_size) for logits\n",
    "\n",
    "            # this preserves the each sequence in the batch, and the vocab logits, but only the last token in each sequence\n",
    "            logits = logits[:, -1, :]                           # (batch, vocab_size)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)                   # (batch, vocab_size)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)             # (batch, seq_len + 1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape, loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "logits = model.generate(idx, max_new_tokens=100)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.451451063156128\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(train_data)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FRI yoppe stimigaser, fath,VAs th\n",
      "To t-o anofoveouick.\n",
      "\n",
      "PENI s\n",
      "Ththet g;\n",
      "Ale bl wano pro anthoor a daveathornais fuknonalil me pofove sterdesasthousour wetshe t cran\n",
      "Ant,\n",
      "Bur,\n",
      "Try merh lat ous, pt?\n",
      "AGBY: minord hanoras ke y cu\n",
      "\n",
      "LUCaven cere t VI:\n",
      "\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "logits = model.generate(idx, max_new_tokens=250)\n",
    "print(tokenizer.decode(logits[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "\n",
    "        for step in range(100):\n",
    "            X, Y = get_batch(train_data if split == 'train' else test_data)\n",
    "            _, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        out[split] = sum(losses) / len(losses)\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data, test_data, n_steps=1000):\n",
    "    for step in range(n_steps):\n",
    "        model.train()\n",
    "        X, Y = get_batch(train_data)\n",
    "\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(estimate_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_data, test_data, n_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transLens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
